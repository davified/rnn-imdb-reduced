{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can use a pre-trained word embedding, from a much larger corpus, trained for a much longer period. \n",
    "\n",
    "Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "NB: If you don't have the required data, and the RedCatLabs server doesn't give you the download, the loader below downloads a 823Mb file via a fairly slow connection to a server at Stanford (this can take HOURS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glove\n",
    "import numpy as np\n",
    "\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVE available locally\n"
     ]
    }
   ],
   "source": [
    "import os, requests, shutil\n",
    "import glove\n",
    "\n",
    "glove_dir = './data/RNN/'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "# These are temporary files if we need to download it from the original source (slow)\n",
    "data_cache = './data/cache'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "print(\"GloVE available locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
    "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
    "word_embedding.word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded that, play around with the similarity and analogy tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-analogy test\n",
    "def get_embedding_vec(word):\n",
    "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
    "    if idx<0:\n",
    "        #print(\"Missing word : '%s'\" % (word,))\n",
    "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
    "    return word_embedding.word_vectors[idx]\n",
    "\n",
    "def get_closest_word(vec, number=5):\n",
    "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
    "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
    "                   / np.linalg.norm(vec))\n",
    "    word_ids = np.argsort(-dst)\n",
    "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
    "            if x in word_embedding.inverse_dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(s='one two three four'):\n",
    "    (a,b,c,d) = s.split(' ')\n",
    "    analogy_vec = get_embedding_vec(b) - get_embedding_vec(a) + get_embedding_vec(c)\n",
    "    words = [ w for (w,p) in get_closest_word(analogy_vec) if w not in (a,b,c)]\n",
    "    print(\"'%s' is to '%s' as '%s' is to {%s}\" % (a,b,c,', '.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('another', 0.95155760155240554), ('only', 0.93775685546916843), ('same', 0.93508186215121814), ('.', 0.91384127912218194)]\n",
      "[('why', 0.9699442918090454), ('how', 0.96343385789841574), ('nothing', 0.95298521021645888), ('something', 0.95179124988244079)]\n",
      "[('prince', 0.8236179693335699), ('queen', 0.78390430109641174), ('ii', 0.77462300306351073), ('emperor', 0.77362476248729251)]\n",
      "[('summer', 0.91998354386035297), ('spring', 0.86244882945357526), ('autumn', 0.84603370064260131), ('rainy', 0.77108566972678583)]\n",
      "[('meat', 0.90898847625037249), ('fried', 0.8994253485084005), ('pork', 0.89203300290774223), ('soup', 0.86930858078715156)]\n"
     ]
    }
   ],
   "source": [
    "test_words = ['one', 'what', 'king', 'winter', 'chicken']\n",
    "for word in test_words:\n",
    "    print(word_embedding.most_similar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'man' is to 'woman' as 'king' is to {queen, daughter, prince, throne}\n",
      "'paris' is to 'france' as 'rome' is to {italy, spain, portugal}\n",
      "'kitten' is to 'cat' as 'puppy' is to {dog, rabbit, horse}\n",
      "'understand' is to 'understood' as 'run' is to {ran, running, runs, twice}\n"
     ]
    }
   ],
   "source": [
    "test_analogy('man woman king queen')\n",
    "test_analogy('paris france rome italy')\n",
    "test_analogy('kitten cat puppy dog')\n",
    "test_analogy('understand understood run ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "embedding_var = tf.Variable(word_embedding.word_vectors, dtype='float32', name='word_embedding')\n",
    "\n",
    "projector_config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = projector_config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "LOG_DIR='./tensorflow.logdir/'\n",
    "metadata_file = 'glove_full_50d.words.tsv'\n",
    "vocab_list = [ word_embedding.inverse_dictionary[i] \n",
    "               for i in range(len( word_embedding.inverse_dictionary )) ]\n",
    "with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n",
    "    metadata.writelines(\"%s\\n\" % w for w in vocab_list)\n",
    "    \n",
    "embedding.metadata_path = os.path.join(os.getcwd(), LOG_DIR, metadata_file)\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, projector_config)\n",
    "\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the model\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 0.1.6 at http://sg-15.local:6006 (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "# Go to your terminal and run the following command: `tensorboard --logdir=./tensorflow.logdir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
